"""
UCF-101æ•°æ®é›†åŠ è½½å™¨
ä½œç”¨ï¼šåŠ è½½è§†é¢‘æ•°æ®ï¼Œè¿›è¡Œé¢„å¤„ç†ï¼Œæ”¯æŒè®­ç»ƒå’ŒéªŒè¯
"""
import os
import cv2
import torch
import numpy as np
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
import random
from pathlib import Path

class UCF101Dataset(Dataset):
    def __init__(self, data_root, split_file, num_segments=8, transform=None, is_train=True, augment_factor=1):
        """
        Args:
            data_root: UCF-101æ•°æ®é›†æ ¹ç›®å½•
            split_file: åˆ†å‰²æ–‡ä»¶è·¯å¾„ (trainlist01.txt æˆ– testlist01.txt)
            num_segments: æ¯ä¸ªè§†é¢‘é‡‡æ ·çš„å¸§æ•°
            transform: æ•°æ®å¢å¼ºå˜æ¢
            is_train: æ˜¯å¦ä¸ºè®­ç»ƒæ¨¡å¼
            augment_factor: è®­ç»ƒæ—¶æ¯ä¸ªè§†é¢‘çš„å¢å¼ºå€æ•°ï¼ˆæ­¤å¤„å¿½ç•¥ï¼Œå›ºå®šä¸º1ï¼‰
        """
        self.data_root = Path(data_root)
        self.num_segments = num_segments
        self.transform = transform
        self.is_train = is_train
        # å»æ‰å¢å¼ºï¼šè®­ç»ƒä¸éªŒè¯å‡ä¸åšæ ·æœ¬å€å¢
        self.augment_factor = 1

        # åŠ è½½ç±»åˆ«æ˜ å°„
        self.class_to_idx = self._load_class_mapping()

        # åŠ è½½è§†é¢‘åˆ—è¡¨
        self.video_list = self._load_video_list(split_file)

        # ä¸ä½¿ç”¨ç´¯åŠ ç­–ç•¥ï¼šæ ·æœ¬æ€»æ•°ç­‰äºè§†é¢‘æ•°
        self.total_samples = len(self.video_list)

        print(f"ğŸ“Š {'è®­ç»ƒ' if is_train else 'éªŒè¯'}é›†: {len(self.video_list)} ä¸ªè§†é¢‘ï¼ˆæ— æ•°æ®å¢å¼ºï¼‰")
    
    def _load_class_mapping(self):
        """åŠ è½½ç±»åˆ«æ˜ å°„"""
        class_file = Path("ucfTrainTestlist/classInd.txt")
        class_to_idx = {}
        
        with open(class_file, 'r') as f:
            for line in f:
                idx, class_name = line.strip().split()
                class_to_idx[class_name] = int(idx) - 1  # è½¬æ¢ä¸º0-basedç´¢å¼•
        
        return class_to_idx
    
    def _load_video_list(self, split_file):
        """åŠ è½½è§†é¢‘åˆ—è¡¨"""
        video_list = []
        
        with open(split_file, 'r') as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                
                if self.is_train:
                    # è®­ç»ƒé›†æ ¼å¼: video_path class_index
                    parts = line.split()
                    video_path = parts[0]
                    label = int(parts[1]) - 1  # è½¬æ¢ä¸º0-based
                else:
                    # æµ‹è¯•é›†æ ¼å¼: video_path (éœ€è¦ä»è·¯å¾„æ¨æ–­æ ‡ç­¾)
                    video_path = line
                    class_name = video_path.split('/')[0]
                    label = self.class_to_idx[class_name]
                
                # æ£€æŸ¥è§†é¢‘æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                full_path = self.data_root / video_path
                if full_path.exists():
                    video_list.append((video_path, label))
                else:
                    print(f"âš ï¸  è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {full_path}")
        
        return video_list
    
    def _load_video(self, video_path):
        """åŠ è½½è§†é¢‘å¹¶é‡‡æ ·å¸§"""
        full_path = self.data_root / video_path
        
        # ä½¿ç”¨OpenCVè¯»å–è§†é¢‘
        cap = cv2.VideoCapture(str(full_path))
        frames = []
        
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            # è½¬æ¢BGRåˆ°RGB
            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            frames.append(frame)
        
        cap.release()
        
        if len(frames) == 0:
            raise ValueError(f"æ— æ³•è¯»å–è§†é¢‘: {video_path}")
        
        # TSNé£æ ¼éšæœº/ä¸­å¿ƒé‡‡æ ·æŒ‡å®šæ•°é‡çš„å¸§
        return self._sample_frames(frames)
    
    def _sample_frames(self, frames):
        """
        TSNé£æ ¼çš„éšæœºé‡‡æ ·ï¼šå°†è§†é¢‘åˆ†ä¸ºnum_segmentsä¸ªç‰‡æ®µï¼Œæ¯ä¸ªç‰‡æ®µå†…éšæœºé€‰æ‹©ä¸€å¸§
        """
        total_frames = len(frames)
        
        if total_frames <= self.num_segments:
            # ğŸ”¥ ä¿®å¤çŸ­è§†é¢‘é—®é¢˜ï¼šä½¿ç”¨æ›´æ™ºèƒ½çš„é‡å¤ç­–ç•¥
            indices = []
            # å…ˆå‡åŒ€åˆ†å¸ƒç°æœ‰å¸§
            step = total_frames / self.num_segments
            for i in range(self.num_segments):
                idx = min(int(i * step), total_frames - 1)
                indices.append(idx)
        else:
            # ğŸ² TSNé£æ ¼éšæœºé‡‡æ ·
            # å°†è§†é¢‘åˆ†ä¸ºnum_segmentsä¸ªç‰‡æ®µï¼Œæ¯ä¸ªç‰‡æ®µå†…éšæœºé€‰æ‹©ä¸€å¸§
            indices = []
            segment_length = total_frames / self.num_segments
            
            for i in range(self.num_segments):
                # è®¡ç®—å½“å‰ç‰‡æ®µçš„èµ·å§‹å’Œç»“æŸä½ç½®
                start_idx = int(i * segment_length)
                end_idx = int((i + 1) * segment_length)
                
                # ç¡®ä¿ä¸è¶Šç•Œ
                end_idx = min(end_idx, total_frames)
                
                if start_idx >= end_idx:
                    # è¾¹ç•Œæƒ…å†µï¼Œä½¿ç”¨start_idx
                    indices.append(start_idx - 1 if start_idx > 0 else 0)
                else:
                    # åœ¨å½“å‰ç‰‡æ®µå†…éšæœºé€‰æ‹©ä¸€å¸§
                    if self.is_train:
                        # è®­ç»ƒæ—¶éšæœº
                        random_idx = random.randint(start_idx, end_idx - 1)
                    else:
                        # éªŒè¯æ—¶å›ºå®šé€‰æ‹©ä¸­é—´å¸§ï¼Œå¢åŠ ä¸€è‡´æ€§
                        random_idx = start_idx + (end_idx - start_idx) // 2
                    indices.append(random_idx)
        
        sampled_frames = [frames[i] for i in indices]
        return sampled_frames
    
    def __len__(self):
        return self.total_samples
    
    def __getitem__(self, idx):
        # ç®€åŒ–ï¼šæ— å¢å¼ºï¼Œç›´æ¥æŒ‰ç´¢å¼•å–è§†é¢‘
        video_idx = idx
        
        video_path, label = self.video_list[video_idx]
        
        try:
            # åŠ è½½è§†é¢‘å¸§
            frames = self._load_video(video_path)
            
            # å•ä¸€å˜æ¢ï¼ˆè®­ç»ƒ/éªŒè¯ä¸€è‡´ï¼‰
            frames = [self.transform(frame) for frame in frames]
            
            # å°†å¸§åˆ—è¡¨è½¬æ¢ä¸ºå¼ é‡ (T, C, H, W)
            video_tensor = torch.stack(frames)
            
            return video_tensor, label
            
        except Exception as e:
            print(f"âŒ åŠ è½½è§†é¢‘å¤±è´¥: {video_path} (æ ·æœ¬{idx}), é”™è¯¯: {e}")
            # è¿”å›ç¬¬ä¸€ä¸ªè§†é¢‘ä½œä¸ºé»˜è®¤å€¼
            return self.__getitem__(0)

def get_transforms(is_train=True):
    """
    ç®€åŒ–ç‰ˆæ•°æ®å˜æ¢ï¼ˆæ— æ•°æ®å¢å¼ºï¼Œè®­ç»ƒ/éªŒè¯ä¸€è‡´ï¼‰
    """
    transform = transforms.Compose([
        transforms.ToPILImage(),
    # ä¿æŒçºµæ¨ªæ¯”ï¼šå°†çŸ­è¾¹ç¼©æ”¾åˆ°256ï¼Œå†ä¸­å¿ƒè£å‰ªåˆ°224ï¼Œé¿å…æ‹‰ä¼¸å˜å½¢
    transforms.Resize(256),
        transforms.CenterCrop((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                           std=[0.229, 0.224, 0.225])
    ])
    return transform

def custom_collate_fn(batch):
    """è‡ªå®šä¹‰æ‰¹æ¬¡å¤„ç†å‡½æ•°"""
    videos, labels = zip(*batch)
    
    # å°†è§†é¢‘æ•°æ®é‡å¡‘ä¸ºTEAæ¨¡å‹éœ€è¦çš„æ ¼å¼
    batch_size = len(videos)
    num_segments = videos[0].size(0)
    
    # (B, T, C, H, W) -> (B*T, C, H, W)
    videos = torch.stack(videos)  # (B, T, C, H, W)
    videos = videos.view(-1, videos.size(2), videos.size(3), videos.size(4))  # (B*T, C, H, W)
    
    labels = torch.LongTensor(labels)
    
    return videos, labels

def get_ucf101_loaders(config):
    """åˆ›å»ºUCF-101æ•°æ®åŠ è½½å™¨"""
    
    # æ•°æ®å˜æ¢
    # è®­ç»ƒ/éªŒè¯ä½¿ç”¨åŒä¸€å¥—æ— å¢å¼ºå˜æ¢
    train_transform = get_transforms(is_train=True)
    val_transform = get_transforms(is_train=False)
    
    # æ•°æ®é›†
    train_dataset = UCF101Dataset(
        data_root=config.data_root,
        split_file=f"ucfTrainTestlist/trainlist0{config.split}.txt",
        num_segments=config.num_segments,
    transform=train_transform,
    is_train=True,
    augment_factor=1
    )
    
    val_dataset = UCF101Dataset(
        data_root=config.data_root,
        split_file=f"ucfTrainTestlist/testlist0{config.split}.txt",
        num_segments=config.num_segments,
    transform=val_transform,
    is_train=False,
    augment_factor=1
    )
    
    # æ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(
        train_dataset,
        batch_size=config.batch_size,
        shuffle=True,
        num_workers=config.num_workers,
        pin_memory=config.pin_memory,
        collate_fn=custom_collate_fn,
        drop_last=True
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=config.batch_size,
        shuffle=False,
        num_workers=config.num_workers,
        pin_memory=config.pin_memory,
        collate_fn=custom_collate_fn,
        drop_last=False
    )
    
    print(f"âœ… æ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæˆ!")
    print(f"   è®­ç»ƒæ‰¹æ¬¡æ•°: {len(train_loader)}")
    print(f"   éªŒè¯æ‰¹æ¬¡æ•°: {len(val_loader)}")
    
    return train_loader, val_loader

# æµ‹è¯•æ•°æ®åŠ è½½å™¨
if __name__ == "__main__":
    from ConfigBlock import get_config
    
    config = get_config()
    config.data_root = "UCF-101"  # æ ¹æ®ä½ çš„è·¯å¾„è°ƒæ•´
    config.batch_size = 2  # æµ‹è¯•ç”¨å°batch
    
    try:
        train_loader, val_loader = get_ucf101_loaders(config)
        
        # æµ‹è¯•åŠ è½½ä¸€ä¸ªæ‰¹æ¬¡
        for videos, labels in train_loader:
            print(f"è§†é¢‘æ•°æ®å½¢çŠ¶: {videos.shape}")
            print(f"æ ‡ç­¾å½¢çŠ¶: {labels.shape}")
            print(f"æ ‡ç­¾å€¼: {labels}")
            break
            
        print("ğŸ‰ æ•°æ®åŠ è½½å™¨æµ‹è¯•æˆåŠŸ!")
        
    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½å™¨æµ‹è¯•å¤±è´¥: {e}")