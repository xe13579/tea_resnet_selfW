"""
TEAè®­ç»ƒå™¨æ¨¡å—
ä½œç”¨ï¼šå°è£…è®­ç»ƒé€»è¾‘ï¼Œæ”¯æŒæ£€æŸ¥ç‚¹ä¿å­˜/åŠ è½½ï¼Œæ—¥å¿—è®°å½•ç­‰
"""
import os
import time
import math
from pathlib import Path

import torch
import torch.nn as nn
import torch.optim as optim


class TEATrainer:
    def __init__(self, model, train_loader, val_loader, config):
        self.model = model
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.config = config

        # è®¾å¤‡é…ç½®
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model.to(self.device)
        print(f"ğŸš€ ä½¿ç”¨è®¾å¤‡: {self.device}")

        # æŸå¤±å‡½æ•°ï¼ˆå…³é—­ label smoothingï¼‰
        self.criterion = nn.CrossEntropyLoss(label_smoothing=0.0)

        # ä¼˜åŒ–å™¨/è°ƒåº¦å™¨
        self._setup_optimizer()

        # ä¸ä½¿ç”¨æ¢¯åº¦è£å‰ª
        self.grad_clip = None

        # è®­ç»ƒçŠ¶æ€
        self.start_epoch = 0
        self.best_acc = 0.0
        self.train_losses = []
        self.val_losses = []
        self.train_accs = []
        self.val_accs = []

        # åˆ›å»ºä¿å­˜ç›®å½•
        self.save_dir = Path(config.save_dir)
        self.save_dir.mkdir(exist_ok=True)

    def _setup_optimizer(self):
        """è®¾ç½®ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨"""
        self.scheduler = None

        if getattr(self.config, 'optimizer', 'adam') == 'sgd':
            self.optimizer = optim.SGD(
                self.model.parameters(),
                lr=self.config.lr,
                momentum=getattr(self.config, 'momentum', 0.9),
                weight_decay=self.config.weight_decay,
            )
        else:
            self.optimizer = optim.Adam(
                self.model.parameters(),
                lr=self.config.lr,
                weight_decay=self.config.weight_decay,
            )

        sched = getattr(self.config, 'scheduler', None)
        if sched == 'step':
            self.scheduler = optim.lr_scheduler.StepLR(
                self.optimizer,
                step_size=self.config.step_size,
                gamma=self.config.gamma,
            )
        elif sched == 'cosine':
            self.scheduler = optim.lr_scheduler.CosineAnnealingLR(
                self.optimizer,
                T_max=self.config.epochs,
            )

    def save_checkpoint(self, epoch, is_best=False):
        """ä¿å­˜è®­ç»ƒæ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict() if self.scheduler is not None else None,
            'best_acc': self.best_acc,
            'train_losses': self.train_losses,
            'val_losses': self.val_losses,
            'train_accs': self.train_accs,
            'val_accs': self.val_accs,
            'config': self.config,
        }

        latest_path = self.save_dir / 'checkpoint_latest.pth'
        torch.save(checkpoint, latest_path)

        if is_best:
            best_path = self.save_dir / 'checkpoint_best.pth'
            torch.save(checkpoint, best_path)
            print(f"ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹! éªŒè¯ç²¾åº¦: {self.best_acc:.2f}%")

    def load_checkpoint(self, checkpoint_path, reset_optimizer=False):
        """åŠ è½½æ£€æŸ¥ç‚¹ï¼Œæ”¯æŒé€‰æ‹©æ˜¯å¦é‡ç½®ä¼˜åŒ–å™¨/è°ƒåº¦å™¨çŠ¶æ€"""
        print(f"ğŸ“¥ åŠ è½½æ£€æŸ¥ç‚¹: {checkpoint_path}")

        if not os.path.exists(checkpoint_path):
            print(f"âŒ æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
            return 0

        try:
            checkpoint = torch.load(checkpoint_path, map_location=self.device, weights_only=False)

            # æ¨¡å‹
            state = checkpoint.get('model_state_dict', checkpoint)
            self.model.load_state_dict(state)
            print("âœ… æ¨¡å‹æƒé‡åŠ è½½æˆåŠŸ")

            if not reset_optimizer and 'optimizer_state_dict' in checkpoint:
                self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
                print("âœ… ä¼˜åŒ–å™¨çŠ¶æ€åŠ è½½æˆåŠŸ")

                sched_state = checkpoint.get('scheduler_state_dict', None)
                if self.scheduler is not None and sched_state is not None:
                    self.scheduler.load_state_dict(sched_state)
                    print("âœ… è°ƒåº¦å™¨çŠ¶æ€åŠ è½½æˆåŠŸ")

                start_epoch = int(checkpoint.get('epoch', 0)) + 1
                print(f"ğŸ”„ ç»§ç»­è®­ç»ƒï¼Œä»ç¬¬{start_epoch}è½®å¼€å§‹")
            else:
                print("ğŸ”„ é‡ç½®ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨çŠ¶æ€")
                print(f"ğŸ“Š æ–°çš„åˆå§‹å­¦ä¹ ç‡: {self.optimizer.param_groups[0]['lr']}")
                start_epoch = 0

            # å†å²ä¿¡æ¯
            prev_best = float(checkpoint.get('best_acc', 0.0))
            self.best_acc = prev_best
            self.train_losses = checkpoint.get('train_losses', []) or []
            self.val_losses = checkpoint.get('val_losses', []) or []
            self.train_accs = checkpoint.get('train_accs', []) or []
            self.val_accs = checkpoint.get('val_accs', []) or []

            self.start_epoch = start_epoch
            print(f"ğŸ“Š ä¹‹å‰çš„æœ€ä½³ç²¾åº¦: {prev_best:.2f}% (ä½œä¸ºèµ·ç‚¹)")

            return start_epoch
        except Exception as e:
            print(f"âŒ æ£€æŸ¥ç‚¹åŠ è½½å¤±è´¥: {e}")
            return 0

    def train_epoch(self, epoch):
        """å•ä¸ªepochè®­ç»ƒ"""
        self.model.train()
        total_loss = 0.0
        correct = 0
        total = 0

        for batch_idx, (data, target) in enumerate(self.train_loader):
            data, target = data.to(self.device), target.to(self.device)

            # å‰å‘/åå‘
            self.optimizer.zero_grad(set_to_none=True)
            output = self.model(data)
            loss = self.criterion(output, target)
            loss.backward()

            # ç»Ÿè®¡æ¢¯åº¦èŒƒæ•°ï¼ˆæ— è£å‰ªï¼Œä»…ç”¨äºè§‚å¯Ÿï¼‰
            total_sq = 0.0
            for p in self.model.parameters():
                if p.grad is not None:
                    total_sq += p.grad.detach().pow(2).sum().item()
            grad_norm = math.sqrt(total_sq)

            # æ— è£å‰ª
            self.optimizer.step()

            # ç»Ÿè®¡
            total_loss += loss.item()
            _, predicted = output.max(1)
            total += target.size(0)
            correct += predicted.eq(target).sum().item()

            if batch_idx % self.config.print_freq == 0:
                print(
                    f"Epoch {epoch+1}/{self.config.epochs} | "
                    f"Batch {batch_idx}/{len(self.train_loader)} | "
                    f"Loss: {loss.item():.4f} | "
                    f"Acc: {100.*correct/total:.2f}% | "
                    f"Grad: {grad_norm:.4f} | "
                    f"LR: {self.optimizer.param_groups[0]['lr']:.6f}"
                )

        avg_loss = total_loss / max(1, len(self.train_loader))
        accuracy = 100.0 * correct / max(1, total)
        return avg_loss, accuracy

    def validate(self):
        """éªŒè¯å‡½æ•°"""
        self.model.eval()
        total_loss = 0.0
        correct = 0
        total = 0

        with torch.no_grad():
            for data, target in self.val_loader:
                data, target = data.to(self.device), target.to(self.device)
                output = self.model(data)
                loss = self.criterion(output, target)

                total_loss += loss.item()
                _, predicted = output.max(1)
                total += target.size(0)
                correct += predicted.eq(target).sum().item()

        avg_loss = total_loss / max(1, len(self.val_loader))
        accuracy = 100.0 * correct / max(1, total)
        return avg_loss, accuracy

    def evaluate_loader(self, loader):
        """åœ¨ eval æ¨¡å¼ä¸‹è¯„ä¼°ä»»æ„ DataLoaderï¼ˆå¦‚è®­ç»ƒé›†ï¼‰"""
        self.model.eval()
        total_loss = 0.0
        correct = 0
        total = 0

        with torch.no_grad():
            for data, target in loader:
                data, target = data.to(self.device), target.to(self.device)
                output = self.model(data)
                loss = self.criterion(output, target)

                total_loss += loss.item()
                _, predicted = output.max(1)
                total += target.size(0)
                correct += predicted.eq(target).sum().item()

        avg_loss = total_loss / max(1, len(loader))
        accuracy = 100.0 * correct / max(1, total)
        return avg_loss, accuracy

    def train(self):
        """ä¸»è®­ç»ƒå¾ªç¯"""
        print("ğŸ¯ å¼€å§‹è®­ç»ƒ...")
        print(f"è®­ç»ƒå‚æ•°: {vars(self.config)}")
        print("-" * 80)

        for epoch in range(self.start_epoch, self.config.epochs):
            start_time = time.time()

            train_loss, train_acc = self.train_epoch(epoch)
            val_loss, val_acc = self.validate()

            if self.scheduler is not None:
                self.scheduler.step()

            self.train_losses.append(train_loss)
            self.val_losses.append(val_loss)
            self.train_accs.append(train_acc)
            self.val_accs.append(val_acc)

            epoch_time = time.time() - start_time

            print(f"\nğŸ“Š Epoch {epoch+1}/{self.config.epochs} ç»“æœ:")
            print(f"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%")
            print(f"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%")
            print(f"Time: {epoch_time:.2f}s | LR: {self.optimizer.param_groups[0]['lr']:.6f}")

            is_best = val_acc > self.best_acc
            if is_best:
                self.best_acc = val_acc
            self.save_checkpoint(epoch, is_best)

            print("-" * 80)

        print(f"ğŸ‰ è®­ç»ƒå®Œæˆ! æœ€ä½³éªŒè¯ç²¾åº¦: {self.best_acc:.2f}%")
        self.save_checkpoint(self.config.epochs - 1, False)
        return self.best_acc